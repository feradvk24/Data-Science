{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ecac9c20-e9eb-4b83-b9b3-edd1ec32d57a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your imports here\n",
    "from skimage.io import imread_collection\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from collections import defaultdict\n",
    "from skimage.transform import resize\n",
    "import shutil\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from tensorflow.image import rgb_to_grayscale, grayscale_to_rgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f409693-729b-4a8a-b69e-767deb80b648",
   "metadata": {},
   "source": [
    "# Working with Images Lab\n",
    "## Information retrieval, preprocessing, and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a328e6-f43f-40c3-abf3-cb650ff59141",
   "metadata": {},
   "source": [
    "In this lab, you'll work with images of felines (cats), which have been classified according to their taxonomy. Each subfolder contains images of a particular species. The dataset is located [here](https://www.kaggle.com/datasets/datahmifitb/felis-taxonomy-image-classification) but it's also provided to you in the `data/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930cdea-105b-4f27-b28f-30323036b6c1",
   "metadata": {},
   "source": [
    "### Problem 1. Some exploration (1 point)\n",
    "How many types of cats are there? How many images do we have of each? What is a typical image size? Are there any outliers in size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33f4fd-5ae9-435c-8e10-3ee3079d7992",
   "metadata": {},
   "source": [
    "Lets create a list that contains a collection for each folder in the 'data' folder. To do so lets use the 'os' module and get a list of all subfolders, then create the collections list and append a collection for each of the subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72d6fbdb-37b4-4ce4-a3fa-65954c377539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of cats: 7\n",
      "['african wildcat', 'blackfoot cat', 'chinese mountain cat', 'domestic cat', 'european wildcat', 'jungle cat', 'sand cat']\n",
      "african wildcat, Number of images: 91\n",
      "blackfoot cat, Number of images: 79\n",
      "chinese mountain cat, Number of images: 42\n",
      "domestic cat, Number of images: 64\n",
      "european wildcat, Number of images: 85\n",
      "jungle cat, Number of images: 86\n",
      "sand cat, Number of images: 72\n"
     ]
    }
   ],
   "source": [
    "def create_imread_collections(subfolders):\n",
    "    \"\"\"Creates a list of imread_collection objects for each subfolder.\"\"\"\n",
    "    collections = []\n",
    "    for subfolder in subfolders:\n",
    "        image_files = [os.path.join(subfolder, filename) \n",
    "                       for filename in os.listdir(subfolder) \n",
    "                       if filename.lower().endswith(('.jpg', '.jpeg'))]\n",
    "        collection = imread_collection(image_files)\n",
    "        collections.append(collection)\n",
    "    return collections\n",
    "\n",
    "main_folder = 'data' #Load the main folder\n",
    "#Get a list of all subfolders\n",
    "subfolders = [os.path.join(main_folder, subfolder) \n",
    "              for subfolder in os.listdir(main_folder) \n",
    "              if os.path.isdir(os.path.join(main_folder, subfolder))] \n",
    "cat_collections = create_imread_collections(subfolders) #Create the list of collections\n",
    "\n",
    "print(\"Types of cats: \" + str(len(subfolders)))\n",
    "subfolders_name = [i.replace(\"data\\\\\", \"\").replace(\"-\", \" \") for i in subfolders] #Clean the names of the subfolders\n",
    "    \n",
    "print(subfolders_name)\n",
    "# Print out the number of images in each collection for verification\n",
    "for i, collection in enumerate(cat_collections):\n",
    "    print(f'{subfolders_name[i]}, Number of images: {len(collection)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0e2de-0918-441a-989b-950f3ed3af2c",
   "metadata": {},
   "source": [
    "The most common image size seems to be around 275x183, but there are many outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54a788-a46a-4683-b78e-6e9c9e25c46d",
   "metadata": {},
   "source": [
    "### Problem 2. Duplicat(e)s (1 point)\n",
    "Find a way to filter out (remove) identical images. I would recommnend using file hashes, but there are many approaches. Keep in mind that during file saving, recompression, etc., a lot of artifacts can change the file content (bytes), but not visually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085a47b-4eb5-4bb2-aac4-87773c8f947c",
   "metadata": {},
   "source": [
    "To remove the duplicate images, we will use perceptual hashing to detect and filter out images with identical visual content. The image hashes have to be computed using **'imagehash'**, but to do that, each image has to be converted to a PIL image, which allows to use the **'PIL'** module proccessing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a8920c-1237-488e-8267-c1f6cadbdb38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_duplicates_from_collection(collection):\n",
    "    \"\"\"Filters out duplicate images from the given imread_collection.\"\"\"\n",
    "    seen_hashes = set()\n",
    "    unique_images = []\n",
    "    \n",
    "    for image in collection:\n",
    "        # Convert image to PIL Image for hashing\n",
    "        pil_image = Image.fromarray(image)\n",
    "        img_hash = imagehash.average_hash(pil_image)\n",
    "        \n",
    "        #Add the image to the unique_images list if its hash is not in the seen_hashes set\n",
    "        if img_hash not in seen_hashes:\n",
    "            seen_hashes.add(img_hash)\n",
    "            unique_images.append(image)\n",
    "    \n",
    "    return unique_images\n",
    "\n",
    "def filter_duplicates(collections):\n",
    "    \"\"\"Filters duplicates from each collection and returns the list of filtered collections.\"\"\"\n",
    "    filtered_collections = []\n",
    "    for collection in collections:\n",
    "        unique_images = filter_duplicates_from_collection(collection)\n",
    "        filtered_collections.append(unique_images)\n",
    "    return filtered_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cead599-d43e-4136-89c9-c93903c0557c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "african wildcat, Number of unique images: 89\n",
      "blackfoot cat, Number of unique images: 78\n",
      "chinese mountain cat, Number of unique images: 37\n",
      "domestic cat, Number of unique images: 60\n",
      "european wildcat, Number of unique images: 60\n",
      "jungle cat, Number of unique images: 80\n",
      "sand cat, Number of unique images: 70\n"
     ]
    }
   ],
   "source": [
    "filtered_collections = filter_duplicates(cat_collections)\n",
    "\n",
    "# Print out the number of unique images in each filtered collection for verification\n",
    "for i, unique_images in enumerate(filtered_collections):\n",
    "    print(f'{subfolders_name[i]}, Number of unique images: {len(unique_images)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2dc3f5-c241-4046-9d2b-6121d175130f",
   "metadata": {},
   "source": [
    "We can see the number of images in each collection is now lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918359ba-8d97-42c4-82ec-975931ec7fb9",
   "metadata": {},
   "source": [
    "### Problem 3. Loading a model (2 points)\n",
    "Find a suitable, trained convolutional neural network classifier. I recommend `ResNet50` as it's small enough to run well on any machine and powerful enough to make reasonable predictions. Most ready-made classifiers have been trained for 1000 classes.\n",
    "\n",
    "You'll need to install libraries and possibly tinker with configurations for this task. When you're done, display the total number of layers and the total number of parameters. For ResNet50, you should expect around 50 layers and 25M parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ecacc-3c57-4d52-9ad4-4d8074d620ef",
   "metadata": {},
   "source": [
    "We first need to install the **tensorflow** library and import **RestNet50**, **image**, **preprocess_input** and **decode_predictions** from different modules from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c7d40ed-2f0e-42ea-82cc-e3427c84d061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "\n",
    "# Load the pre-trained ResNet50 model\n",
    "model = ResNet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4381e2c6-46c7-4ea6-8bcf-0d90e7ac513c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of layers: 177\n",
      "Total number of parameters: 25636712\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(model.layers)\n",
    "print(f\"Total number of layers: {num_layers}\")\n",
    "num_params = model.count_params()\n",
    "print(f\"Total number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f08200-1dc3-44cf-99ca-f782a5ecbcf8",
   "metadata": {},
   "source": [
    "It shows 177 layers, but as far as i understand, pooling layers (max and average) and the activation functions (ReLU) aren’t counted towards the total layers, so in the end they should be 50. Lets try to search only for the convolutional layers and see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20f4806d-9aaf-4e42-822f-022ba5313c15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of convolutional layers: 53\n"
     ]
    }
   ],
   "source": [
    "conv_layers = [layer for layer in model.layers if isinstance(layer, tf.keras.layers.Conv2D)]\n",
    "print(f\"Number of convolutional layers: {len(conv_layers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef6b32-db23-4616-b497-e78a41a1d487",
   "metadata": {},
   "source": [
    "### Problem 4. Prepare the images (1 point)\n",
    "You'll need to prepare the images for passing to the model. To do so, they have to be resized to the same dimensions. Most available models have a specific requirement for sizes. You may need to do additional preprocessing, depending on the model requirements. These requirements should be easily available in the model documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736d60b-74d2-449e-8177-eff8d80df5b6",
   "metadata": {},
   "source": [
    "Before giving the images to the ResNet50 model, they first need to be prepared to the model-specific requirments. They need to first be resized to 224x224 pixels, then normalized, using the preprocess_input() function from **tensorflow.keras.applications.resnet50** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1a0ff31-3673-40c1-91ec-b12fdd2afe13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = preprocess_input(image)  # Apply ResNet50-specific preprocessing\n",
    "    image = tf.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "def process_images(filtered_collections):\n",
    "    prepared_images = []\n",
    "    for collection in filtered_collections:\n",
    "        for img in collection:\n",
    "            prepared_image = preprocess_image(img)\n",
    "            prepared_images.append(prepared_image)\n",
    "    return prepared_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc5cdc9f-05f9-44f4-8747-501b41f987db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prepared_images = process_images(filtered_collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ade4b3-62ac-4f4e-aa02-3c65dc9ab3d9",
   "metadata": {},
   "source": [
    "### Problem 5. Load the images efficiently (1 point)\n",
    "Now that you've seen how to prepare the images for passing to the model... find a way to do it efficiently. Instead of loading the entire dataset in the RAM, read the images in batches (e.g. 4 images at a time). The goal is to read these, preprocess them, maybe save the preprocessed results in RAM.\n",
    "\n",
    "If you've already done this in one of the previous problems, just skip this one. You'll get your point for it.\n",
    "\n",
    "\\* Even better, save the preprocessed image arrays (they will not be valid .jpg file) as separate files, so you can load them \"lazily\" in the following steps. This is a very common optimization to work with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be228ff-bab8-4bfd-a75d-6c30a4683180",
   "metadata": {},
   "source": [
    "I will use the preprocess_image() function, created earlier, but create a new function that loads only a certain amount of images in a batch, preprocesses them and saves them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70b660eb-ab73-42f1-8f57-34fff32d6111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_images(collection_list, output_dir, batch_size=4):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Flatten the list of collections into a single list of images\n",
    "    image_list = [image for collection in collection_list for image in collection]\n",
    "    \n",
    "    # Delete existing batch files in the directory\n",
    "    for file_name in os.listdir(output_dir):\n",
    "        if file_name.startswith('batch_') and file_name.endswith('.npy'):\n",
    "            os.remove(os.path.join(output_dir, file_name))\n",
    "    \n",
    "    num_images = len(image_list)\n",
    "    for i in range(0, num_images, batch_size):\n",
    "        batch_images = image_list[i:i + batch_size]\n",
    "        preprocessed_images = []\n",
    "\n",
    "        for img in batch_images:\n",
    "            img = preprocess_image(img)\n",
    "            preprocessed_images.append(img)\n",
    "\n",
    "        # Convert the list of images to a NumPy array\n",
    "        image_batch = np.array(preprocessed_images)\n",
    "        \n",
    "        # Preprocess the batch\n",
    "        preprocessed_batch = preprocess_input(image_batch)\n",
    "\n",
    "        # Save each batch to a separate file\n",
    "        batch_filename = os.path.join(output_dir, f'batch_{i // batch_size + 1}.npy')\n",
    "        np.save(batch_filename, preprocessed_batch)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "114180d4-139c-49c5-8426-b7508b7c984f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess and save images in separate files\n",
    "preprocess_and_save_images(filtered_collections, \"preprocessed_batches\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f557e-fae0-42f1-aa3b-4402a9be05d2",
   "metadata": {},
   "source": [
    "### Problem 6. Predictions (1 point)\n",
    "Finally, you're ready to get into the meat of the problem. Obtain predictions from your model and evaluate them. This will likely involve manual work to decide how the returned classes relate to the original ones.\n",
    "\n",
    "Create a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) to evaluate the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d42b2fb7-6cad-4bea-84d5-b4658f15e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(file_path):\n",
    "    # Load a single batch from the .npy file\n",
    "    batch = np.load(file_path)\n",
    "    \n",
    "    # Reshape the batch if it has an extra dimension\n",
    "    if batch.ndim == 5 and batch.shape[1] == 1:\n",
    "        batch = np.squeeze(batch, axis=1)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def make_predictions(batch):\n",
    "    # Make predictions on the batch and decode predictions to readable labels\n",
    "    preds = model.predict(batch)\n",
    "    decoded_preds = decode_predictions(preds, top=3)  # Top-3 predictions\n",
    "    \n",
    "    return decoded_preds\n",
    "\n",
    "def process_batches(directory, batch_prefix='batch_', extension='.npy', max_batches = 10):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    # Filter files that match the batch prefix and extension\n",
    "    batch_files = [f for f in files if f.startswith(batch_prefix) and f.endswith(extension)]\n",
    "    \n",
    "    # Limit the number of batches if max_batches is specified. This leads to only a certain count of batches to be read and displayed\n",
    "    if max_batches is not None:\n",
    "        batch_files = batch_files[:max_batches]\n",
    "    \n",
    "    #Iterate through each batch file\n",
    "    for file_name in batch_files:\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "        # Load the batch and make a prediction\n",
    "        batch = load_batch(file_path)\n",
    "        predictions = make_predictions(batch)\n",
    "            \n",
    "        # Print the predictions\n",
    "        print(f'Predictions for {file_name}:')\n",
    "        for i, preds in enumerate(predictions):\n",
    "            print(f'\\nImage {i + 1}:', end=\"\")\n",
    "            for label, description, score in preds:\n",
    "                print(f'    {description} ({score:.2f})', end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b62ed16a-21a6-4ee1-8fd6-d2d0124f1a06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "Predictions for batch_1.npy:\n",
      "\n",
      "Image 1:    lynx (0.80)    cougar (0.18)    lion (0.01)\n",
      "Image 2:    Egyptian_cat (0.90)    snow_leopard (0.03)    tabby (0.03)\n",
      "Image 3:    lynx (0.38)    tiger_cat (0.35)    Egyptian_cat (0.14)\n",
      "Image 4:    cougar (0.63)    hyena (0.28)    lion (0.03)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step\n",
      "Predictions for batch_10.npy:\n",
      "\n",
      "Image 1:    lynx (0.24)    llama (0.14)    hare (0.07)\n",
      "Image 2:    wood_rabbit (0.24)    gazelle (0.09)    hare (0.06)\n",
      "Image 3:    lynx (0.80)    leopard (0.12)    cheetah (0.04)\n",
      "Image 4:    tiger_cat (0.46)    Egyptian_cat (0.24)    tiger (0.21)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step\n",
      "Predictions for batch_11.npy:\n",
      "\n",
      "Image 1:    lynx (0.85)    cougar (0.04)    tiger_cat (0.03)\n",
      "Image 2:    lynx (0.52)    leopard (0.15)    prairie_chicken (0.09)\n",
      "Image 3:    lynx (0.65)    Egyptian_cat (0.19)    snow_leopard (0.06)\n",
      "Image 4:    lynx (0.42)    coyote (0.22)    hyena (0.15)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step\n",
      "Predictions for batch_12.npy:\n",
      "\n",
      "Image 1:    Egyptian_cat (0.41)    lynx (0.23)    hare (0.06)\n",
      "Image 2:    lynx (0.97)    cougar (0.02)    coyote (0.01)\n",
      "Image 3:    Egyptian_cat (0.25)    mongoose (0.09)    wallaby (0.08)\n",
      "Image 4:    lynx (0.48)    leopard (0.33)    cheetah (0.14)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step\n",
      "Predictions for batch_13.npy:\n",
      "\n",
      "Image 1:    wood_rabbit (0.55)    hare (0.21)    fox_squirrel (0.04)\n",
      "Image 2:    warthog (0.28)    ibex (0.16)    wild_boar (0.13)\n",
      "Image 3:    lynx (0.88)    Egyptian_cat (0.05)    snow_leopard (0.02)\n",
      "Image 4:    badger (0.42)    isopod (0.09)    coyote (0.08)\n",
      "Image 5:    African_hunting_dog (0.77)    hyena (0.07)    dhole (0.02)"
     ]
    }
   ],
   "source": [
    "# Process each batch individually\n",
    "process_batches(\"preprocessed_batches\", max_batches = 5) #Here I will only display the first five batches, containing 5 images each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cec03d-07da-459b-9cf2-803a42590dee",
   "metadata": {},
   "source": [
    "The model seems to be correctly recognizing the images, as most of them get described as different types of cats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020f81e-721f-4882-83eb-80379f7a20ac",
   "metadata": {},
   "source": [
    "### Problem 7. Grayscale (1 point)\n",
    "Converting the images to grayscale should affect the classification negatively, as we lose some of the color information.\n",
    "\n",
    "Find a way to preprocess the images to grayscale (using what you already have in Problem 4 and 5), pass them to the model, and compare the classification results to the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ce5db-3971-43df-aaf9-f95e88c76525",
   "metadata": {},
   "source": [
    "I will use the **tensorflow.image** **rgb_to_grayscale** and **grayscale_to_rgb** functions in order to turn the images to grayscale and then back to 3 channels, because it is required by the ResNet50 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24421ab0-f30f-447c-a496-80d9aa00cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_collections = [[rgb_to_grayscale(image) for image in collection] for collection in filtered_collections]\n",
    "converted_collections = [[grayscale_to_rgb(image) for image in collection] for collection in grayscale_collections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "376eea19-f914-493c-9934-fb16e5967f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_and_save_images(converted_collections, \"grayscale_batches\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea7d2bea-ceab-46ed-90c2-2d941e0422db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step\n",
      "Predictions for batch_1.npy:\n",
      "\n",
      "Image 1:    lynx (0.69)    cougar (0.19)    lion (0.11)\n",
      "Image 2:    Egyptian_cat (0.58)    tiger_cat (0.12)    lynx (0.09)\n",
      "Image 3:    tiger_cat (0.65)    tabby (0.16)    Egyptian_cat (0.13)\n",
      "Image 4:    cougar (0.63)    hyena (0.28)    lion (0.03)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step\n",
      "Predictions for batch_10.npy:\n",
      "\n",
      "Image 1:    zebra (0.26)    llama (0.24)    hartebeest (0.06)\n",
      "Image 2:    gazelle (0.17)    hay (0.07)    lion (0.06)\n",
      "Image 3:    lynx (0.40)    lion (0.29)    cheetah (0.09)\n",
      "Image 4:    tiger_cat (0.52)    Egyptian_cat (0.23)    tiger (0.12)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step\n",
      "Predictions for batch_11.npy:\n",
      "\n",
      "Image 1:    lynx (0.60)    tiger_cat (0.16)    lion (0.07)\n",
      "Image 2:    tiger_cat (0.28)    lynx (0.17)    leopard (0.12)\n",
      "Image 3:    hyena (0.31)    Egyptian_cat (0.30)    lynx (0.08)\n",
      "Image 4:    coyote (0.59)    lion (0.08)    gazelle (0.06)\n",
      "Image 5:    lynx (0.49)    Egyptian_cat (0.32)    tiger_cat (0.08)"
     ]
    }
   ],
   "source": [
    "process_batches(\"grayscale_batches\", max_batches = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49113fd0-b822-4f67-ad91-cd303fb61fab",
   "metadata": {},
   "source": [
    "The predictions look a lot different now. The model still mostly recognizes that the images are cats, but there are differences in the percentage of the categories and some are even changed for another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585e663-2f06-4562-8bea-504b3d583c66",
   "metadata": {},
   "source": [
    "### Problem 8. Deep image features (1 point)\n",
    "Find a way to extract one-dimensional vectors (features) for each (non-grayscale) image, using your model. This is typically done by \"short-circuiting\" the model output to be an intermediate layer, while keeping the input the same. \n",
    "\n",
    "In case the outputs (also called feature maps) have different shapes, you can flatten them in different ways. Try to not create huge vectors; the goal is to have a relatively short sequence of numbers which describes each image.\n",
    "\n",
    "You may find a tutorial like [this](https://towardsdatascience.com/exploring-feature-extraction-with-cnns-345125cefc9a) pretty useful but note your implementation will depend on what model (and framework) you've decided to use.\n",
    "\n",
    "It's a good idea to save these as one or more files, so you'll spare yourself a ton of preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba97e5-964b-43d5-8204-ea27bb7cdb60",
   "metadata": {},
   "source": [
    "Lets systematically processes batches of images from the \"preprocessed_batches\" folder to extract features from them and save them in a human-readable format - csv. We will use the **\"load_batches\"** function from earlier to load images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71347edf-a13b-4683-beb2-af0a093a0cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 577ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 538ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 532ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 547ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 537ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load ResNet50 model with weights pre-trained on ImageNet, excluding the top layer\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add a Global Average Pooling layer to the model\n",
    "gap_layer = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "model = Model(inputs=base_model.input, outputs=gap_layer)\n",
    "\n",
    "# Directory containing the .npy files\n",
    "batch_directory = 'preprocessed_batches'\n",
    "feature_save_directory = 'extracted_features'\n",
    "if os.path.exists(feature_save_directory):\n",
    "    shutil.rmtree(feature_save_directory) #Clear the folder before extracting features\n",
    "    \n",
    "os.makedirs(feature_save_directory, exist_ok=True)\n",
    "\n",
    "# Iterate over each .npy file in the directory\n",
    "for batch_file in os.listdir(batch_directory):\n",
    "    if batch_file.endswith('.npy'):\n",
    "        batch_path = os.path.join(batch_directory, batch_file)\n",
    "        \n",
    "        # Load and preprocess the batch of images\n",
    "        batch = load_batch(batch_path)\n",
    "        if batch is not None:\n",
    "            # Extract features from the batch\n",
    "            features = model.predict(batch)\n",
    "            \n",
    "            # Save the extracted features to a CSV file in the specified directory\n",
    "            feature_file = os.path.join(feature_save_directory, batch_file.replace('.npy', '_features.csv'))\n",
    "            df = pd.DataFrame(features)\n",
    "            df.to_csv(feature_file, index=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd0ab0-910a-4dad-9383-011e7d7616e1",
   "metadata": {},
   "source": [
    "### Problem 9. Putting deep image features to use (1 points)\n",
    "Try to find similar images, using a similarity metric on the features you got in the previous problem. Two good metrics are `mean squared error` and `cosine similarity`. How do they work? Can you spot images that look too similar? Can you explain why?\n",
    "\n",
    "\\* If we were to take Fourier features (in a similar manner, these should be a vector of about the same length), how do they compare to the deep features; i.e., which features are better to \"catch\" similar images?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a466dc63-472c-4f53-8c94-352a41ff7e02",
   "metadata": {},
   "source": [
    "Lets load the preprocessed image feature vectors from the CSV files in **\"extracted_features\"**, combine them into a single matrix, and calculate Mean Squared Error (MSE) and Cosine Similarity. Using these metrics, we can identify and print the top most similar image pairs, with lower MSE values and higher Cosine Similarity values indicating greater similarity between the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a15c714-6b4a-4444-a819-cd1bb1f3a84d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 similar pairs by MSE:\n",
      "Images batch_7_features.csv[2] and batch_51_features.csv[2] have MSE: 0.0004\n",
      "Images batch_51_features.csv[2] and batch_7_features.csv[2] have MSE: 0.0004\n",
      "Images batch_14_features.csv[1] and batch_42_features.csv[3] have MSE: 0.0008\n",
      "Images batch_42_features.csv[3] and batch_14_features.csv[1] have MSE: 0.0008\n",
      "Images batch_61_features.csv[4] and batch_6_features.csv[0] have MSE: 0.0076\n",
      "Images batch_6_features.csv[0] and batch_61_features.csv[4] have MSE: 0.0076\n",
      "Images batch_15_features.csv[1] and batch_4_features.csv[0] have MSE: 0.0100\n",
      "Images batch_4_features.csv[0] and batch_15_features.csv[1] have MSE: 0.0100\n",
      "Images batch_86_features.csv[0] and batch_86_features.csv[2] have MSE: 0.0170\n",
      "Images batch_86_features.csv[2] and batch_86_features.csv[0] have MSE: 0.0170\n",
      "\n",
      "Top 10 similar pairs by Cosine Similarity:\n",
      "Images batch_51_features.csv[2] and batch_7_features.csv[2] have Cosine Similarity: 0.9996\n",
      "Images batch_7_features.csv[2] and batch_51_features.csv[2] have Cosine Similarity: 0.9996\n",
      "Images batch_42_features.csv[3] and batch_14_features.csv[1] have Cosine Similarity: 0.9996\n",
      "Images batch_14_features.csv[1] and batch_42_features.csv[3] have Cosine Similarity: 0.9996\n",
      "Images batch_4_features.csv[0] and batch_15_features.csv[1] have Cosine Similarity: 0.9937\n",
      "Images batch_15_features.csv[1] and batch_4_features.csv[0] have Cosine Similarity: 0.9937\n",
      "Images batch_6_features.csv[0] and batch_61_features.csv[4] have Cosine Similarity: 0.9933\n",
      "Images batch_61_features.csv[4] and batch_6_features.csv[0] have Cosine Similarity: 0.9933\n",
      "Images batch_86_features.csv[0] and batch_86_features.csv[2] have Cosine Similarity: 0.9900\n",
      "Images batch_86_features.csv[2] and batch_86_features.csv[0] have Cosine Similarity: 0.9900\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the extracted feature files\n",
    "feature_directory = 'extracted_features'\n",
    "\n",
    "# Load feature vectors from the CSV files\n",
    "feature_files = [os.path.join(feature_directory, f) for f in os.listdir(feature_directory)]\n",
    "feature_data = []\n",
    "file_indices = []\n",
    "\n",
    "for feature_file in feature_files:\n",
    "    df = pd.read_csv(feature_file)\n",
    "    feature_data.append(df.values)\n",
    "    file_indices.extend([(os.path.basename(feature_file), idx) for idx in range(df.shape[0])])\n",
    "\n",
    "# Combine all features into a single matrix\n",
    "features = np.vstack(feature_data)\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cos_sim_matrix = cosine_similarity(features)\n",
    "\n",
    "# Calculate MSE for each pair of features using broadcasting\n",
    "# Expanding dimensions to broadcast\n",
    "features_expanded = features[:, np.newaxis, :]\n",
    "mse_matrix = np.mean(np.square(features_expanded - features_expanded.transpose(1, 0, 2)), axis=2)\n",
    "\n",
    "# Set diagonals to ignore self-comparison\n",
    "np.fill_diagonal(mse_matrix, np.inf)\n",
    "np.fill_diagonal(cos_sim_matrix, -np.inf)\n",
    "\n",
    "# Flatten the matrices and get the indices of the sorted values\n",
    "mse_indices = np.unravel_index(np.argsort(mse_matrix, axis=None), mse_matrix.shape)\n",
    "cos_sim_indices = np.unravel_index(np.argsort(-cos_sim_matrix, axis=None), cos_sim_matrix.shape)\n",
    "\n",
    "# Get the top 10 pairs for MSE\n",
    "top_10_mse_indices = list(zip(*mse_indices))[:10]\n",
    "top_10_mse = [(file_indices[i], file_indices[j], mse_matrix[i, j]) for i, j in top_10_mse_indices]\n",
    "print(\"Top 10 similar pairs by MSE:\")\n",
    "for pair in top_10_mse:\n",
    "    print(f\"Images {pair[0][0]}[{pair[0][1]}] and {pair[1][0]}[{pair[1][1]}] have MSE: {pair[2]:.4f}\")\n",
    "\n",
    "# Get the top 10 pairs for Cosine Similarity\n",
    "top_10_cos_sim_indices = list(zip(*cos_sim_indices))[:10]\n",
    "top_10_cos_sim = [(file_indices[i], file_indices[j], cos_sim_matrix[i, j]) for i, j in top_10_cos_sim_indices]\n",
    "print(\"\\nTop 10 similar pairs by Cosine Similarity:\")\n",
    "for pair in top_10_cos_sim:\n",
    "    print(f\"Images {pair[0][0]}[{pair[0][1]}] and {pair[1][0]}[{pair[1][1]}] have Cosine Similarity: {pair[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79820d2f-cd6c-4ac0-bdfd-13995e348ec0",
   "metadata": {},
   "source": [
    "The high similarity between some batches is most likely there, because the feature extraction process using ResNet50 might capture similar high-level features for different images that share common patterns, textures, or objects, even if they are not identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba16d333-35f0-4b3d-b4b8-3d86cf5cd8b4",
   "metadata": {},
   "source": [
    "### * Problem 10. Explore, predict, and evaluate further\n",
    "You can do a ton of things here, at your desire. For example, how does masking different areas of the image affect classification - a method known as **saliency map** ([info](https://en.wikipedia.org/wiki/Saliency_map))? Can we detect objects? Can we significantly reduce the number of features (keeping the quality) that we get? Can we reliably train a model to predict our own classes? We'll look into these in detail in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f1a50-ea32-41ce-9215-77a80c300dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
